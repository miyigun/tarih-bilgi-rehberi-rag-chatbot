"""
Veri Ä°ÅŸleme ModÃ¼lÃ¼ - Tarih Bilgi Rehberi
Bu modÃ¼l JSON formatÄ±ndaki tarih verilerini yÃ¼kler, iÅŸler ve FAISSRetriever
sÄ±nÄ±fÄ±nÄ± kullanarak bir FAISS index'i oluÅŸturur ve kaydeder.
"""
import json
from pathlib import Path
from typing import List, Dict
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from utils import clean_text 

class DataProcessor:
    """Tarih verisi iÅŸleme ve embedding oluÅŸturma sÄ±nÄ±fÄ±"""
    
    def __init__(self, data_dir: str = "data/raw", model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):
        """
        Args:
            data_dir: JSON veri dosyalarÄ±nÄ±n bulunduÄŸu dizin
            model_name: KullanÄ±lacak embedding model(Lokal yoldan)
        """
        self.data_dir = Path(data_dir)
        print(f"ğŸ”„ Embedding model yÃ¼kleniyor... (Hugging Face: {model_name})")
        self.embedding_model = SentenceTransformer(model_name)
        self.dimension = self.embedding_model.get_sentence_embedding_dimension()
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=512,
            chunk_overlap=50,
            length_function=len,
            separators=["\n\n", "\n", ". ", "! ", "? ", ", ", " ", ""]
        )
        
    def load_json_data(self) -> List[Document]:
        """
        JSON formatÄ±ndaki tarih verilerini yÃ¼kler ve Document'lere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r
        
        Returns:
            Document nesnelerinin listesi
        """
        documents = []
        
        print(f"ğŸ“‚ Tarih verileri yÃ¼kleniyor: {self.data_dir}")
        
        # JSON dosyalarÄ±nÄ± bul
        json_files = list(self.data_dir.glob('*.json'))
        
        if not json_files:
            print("âš ï¸  HiÃ§ JSON dosyasÄ± bulunamadÄ±!")
            return documents
        
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # JSON yapÄ±sÄ± kontrol et
                if not isinstance(data, list):
                    print(f"  âš ï¸  {json_file.name}: Liste formatÄ±nda deÄŸil, atlanÄ±yor")
                    continue
                
                # Her tarihsel kayÄ±t iÃ§in Document oluÅŸtur
                for item in data:
                    # Veri formatÄ± kontrolÃ¼
                    if not isinstance(item, dict):
                        continue
                    
                    # Ä°Ã§erik oluÅŸtur: konu + icerik + anahtar kelimeler
                    content_parts = []
                    
                    # Konu baÅŸlÄ±ÄŸÄ±
                    if 'konu' in item:
                        content_parts.append(f"Konu: {item['konu']}")
                    
                    # Ana iÃ§erik
                    if 'icerik' in item:
                        content_parts.append(item['icerik'])
                    
                    # Anahtar kelimeleri ekle (arama iÃ§in Ã¶nemli)
                    if 'anahtar_kelimeler' in item and item['anahtar_kelimeler']:
                        keywords = ', '.join(item['anahtar_kelimeler'])
                        content_parts.append(f"Anahtar Kelimeler: {keywords}")
                    
                    # Ä°Ã§eriÄŸi birleÅŸtir
                    content = '\n\n'.join(content_parts)
                    
                    if not content.strip():
                        continue
                    
                    # Metadata oluÅŸtur (zengin metadata RAG iÃ§in kritik)
                    metadata = {
                        'id': item.get('id', 'unknown'),
                        'donem': item.get('donem', ''),
                        'alt_donem': item.get('alt_donem', ''),
                        'kategori_ana': item.get('kategori', {}).get('ana', ''),
                        'kategori_alt': item.get('kategori', {}).get('alt', ''),
                        'konu': item.get('konu', ''),
                        'yil': item.get('yil', 0),
                        'etiketler': item.get('etiketler', []),
                        'kaynak': item.get('kaynak', ''),
                        'kaynak_turu': item.get('kaynak_turu', ''),
                        'referans_link': item.get('referans_link', ''),
                        'source': str(json_file),
                        'filename': json_file.name
                    }
                    
                    # Document oluÅŸtur
                    doc = Document(
                        page_content=content,
                        metadata=metadata
                    )
                    documents.append(doc)
                
                print(f"  âœ“ {json_file.name}: {len([x for x in data if isinstance(x, dict)])} kayÄ±t yÃ¼klendi")
                
            except json.JSONDecodeError as e:
                print(f"  âœ— {json_file.name}: JSON parse hatasÄ± - {str(e)}")
            except Exception as e:
                print(f"  âœ— {json_file.name}: {str(e)}")
        
        print(f"\nâœ… Toplam {len(documents)} tarihsel kayÄ±t yÃ¼klendi\n")
        return documents

    def split_documents(self, documents: List[Document]) -> List[Document]:
        """
        DokÃ¼manlarÄ± chunk'lara bÃ¶ler
        
        Args:
            documents: Document listesi
            
        Returns:
            Chunk'lanmÄ±ÅŸ document listesi
        """
        print("âœ‚ï¸  DokÃ¼manlar chunk'lara bÃ¶lÃ¼nÃ¼yor...")
        
        all_chunks = []
        for doc in documents:
            # Metni temizle (utils'den gelen fonksiyonu kullan)
            cleaned_content = clean_text(doc.page_content)
            doc.page_content = cleaned_content
            
            # Chunk'lara bÃ¶l
            chunks = self.text_splitter.split_documents([doc])
            
            # Her chunk'a orijinal metadata'yÄ± koru
            for chunk in chunks:
                chunk.metadata.update(doc.metadata)
            
            all_chunks.extend(chunks)
        
        print(f"âœ… {len(all_chunks)} chunk oluÅŸturuldu\n")
        return all_chunks
    
    def create_embeddings(self, chunks: List[Document]) -> np.ndarray:
        """
        Chunk'lar iÃ§in embedding vektÃ¶rleri oluÅŸturur
        
        Args:
            chunks: Document chunk'larÄ±
            
        Returns:
            Embedding vektÃ¶rÃ¼ matrisi (n_chunks x dimension)
        """
        print("ğŸ§® Embedding'ler oluÅŸturuluyor...")
        
        texts = [chunk.page_content for chunk in chunks]
        embeddings = self.embedding_model.encode(
            texts,
            show_progress_bar=True,
            batch_size=32
        )
        
        # KosinÃ¼s benzerliÄŸi iÃ§in L2 Normalizasyonu ekle
        embeddings = np.array(embeddings).astype('float32')
        faiss.normalize_L2(embeddings)
        print("âœ… Embedding'ler normalize edildi (L2)")
        
        print(f"âœ… {len(embeddings)} embedding oluÅŸturuldu\n")
        return embeddings
    
    def create_faiss_index(self, embeddings: np.ndarray) -> faiss.Index:
        """
        FAISS index oluÅŸturur
        
        Args:
            embeddings: Embedding vektÃ¶rleri
            
        Returns:
            FAISS index
        """
        print("ğŸ” FAISS index oluÅŸturuluyor...")
        
        # L2 (Euclidean) yerine IP (Inner Product/Cosine) kullan
        index = faiss.IndexFlatIP(self.dimension)
        
        # Embeddings'i ekle
        index.add(embeddings.astype('float32'))
        
        print(f"âœ… FAISS index (IndexFlatIP) oluÅŸturuldu (toplam vektÃ¶r: {index.ntotal})\n")
        return index
    
    def save_index(self, index: faiss.Index, chunks: List[Document], output_dir: str = "models/faiss_index"):
        """
        FAISS index'i ve metadata'yÄ± kaydeder
        
        Args:
            index: FAISS index
            chunks: Document chunk'larÄ±
            output_dir: KayÄ±t dizini
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ’¾ Index kaydediliyor: {output_path}")
        
        # FAISS index'i kaydet
        faiss.write_index(index, str(output_path / "index.faiss"))
        
        # Metadata'yÄ± FAISSRetriever'Ä±n beklediÄŸi gibi
        # doÄŸrudan bir "chunk listesi" olarak kaydet.
        metadata_list = [
            {
                "content": chunk.page_content,
                "metadata": chunk.metadata
            }
            for chunk in chunks
        ]
        
        with open(output_path / "metadata.json", 'w', encoding='utf-8') as f:
            json.dump(metadata_list, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Index ve metadata (chunk listesi) kaydedildi\n")
        
        # Ä°statistikleri ayrÄ± bir dosyaya (veya sadece konsola) yaz
        stats = {
            "total_chunks": len(chunks),
            "dimension": self.dimension,
            "model_name": self.embedding_model._modules['0'].auto_model.name_or_path,
            "data_source": "Tarih Bilgi Rehberi - TÃ¼rk Tarihi",
            "periods": list(set([chunk.metadata.get('donem', '') for chunk in chunks if chunk.metadata.get('donem')]))
        }
        with open(output_path / "stats.json", 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        print("âœ… Ä°statistikler 'stats.json' dosyasÄ±na kaydedildi.")

    
    def process_all(self):
        """TÃ¼m veri iÅŸleme pipeline'Ä±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±r"""
        print("\n" + "="*60)
        print("TARÄ°H BÄ°LGÄ° REHBERÄ° - VERÄ° Ä°ÅLEME PÄ°PELÄ°NE")
        print("="*60 + "\n")
        
        # 1. JSON verilerini yÃ¼kle
        documents = self.load_json_data()
        
        if not documents:
            print("âš ï¸  HiÃ§ veri bulunamadÄ±!")
            print("ğŸ“‹ Beklenen format:")
            print("   data/raw/islamiyet_oncesi.json")
            print("   data/raw/ilk_turk_islam_devletleri.json")
            print("   data/raw/anadolu_beylikleri.json")
            print("   data/raw/osmanli_devleti.json")
            print("   data/raw/milli_mucadele.json")
            print("   data/raw/cumhuriyet.json")
            return
        
        # 2. Chunk'lara bÃ¶l
        chunks = self.split_documents(documents)
        
        # 3. Embedding'leri oluÅŸtur
        embeddings = self.create_embeddings(chunks)
        
        # 4. FAISS index oluÅŸtur
        index = self.create_faiss_index(embeddings)
        
        # 5. Kaydet
        self.save_index(index, chunks)
        
        print("="*60)
        print("âœ… VERÄ° Ä°ÅLEME TAMAMLANDI!")
        print("="*60 + "\n")
        print(f"ğŸ“Š Ä°statistikler:")
        print(f"   â€¢ Toplam tarihsel kayÄ±t: {len(documents)}")
        print(f"   â€¢ Toplam chunk: {len(chunks)}")
        print(f"   â€¢ Embedding boyutu: {self.dimension}")
        print(f"   â€¢ Model: {self.embedding_model._modules['0'].auto_model.name_or_path}")
        
        # DÃ¶nem bazlÄ± istatistikler
        donem_counts = {}
        for chunk in chunks:
            donem = chunk.metadata.get('donem', 'Bilinmiyor')
            donem_counts[donem] = donem_counts.get(donem, 0) + 1
        
        print(f"\nğŸ“š DÃ¶nem BazlÄ± DaÄŸÄ±lÄ±m:")
        for donem, count in sorted(donem_counts.items()):
            print(f"   â€¢ {donem}: {count} chunk")
        print()


def main():
    """Ana fonksiyon"""
    # Veri dizini kontrolÃ¼
    data_dir = Path("data/raw")
    
    if not data_dir.exists():
        print("âš ï¸  data/raw klasÃ¶rÃ¼ bulunamadÄ±. OluÅŸturuluyor...")
        data_dir.mkdir(parents=True, exist_ok=True)
        print("\nğŸ“‹ LÃ¼tfen JSON veri dosyalarÄ±nÄ±zÄ± data/raw/ klasÃ¶rÃ¼ne koyun:")
        print("   â€¢ islamiyet_oncesi.json")
        print("   â€¢ ilk_turk_islam_devletleri.json")
        print("   â€¢ anadolu_beylikleri.json")
        print("   â€¢ osmanli_devleti.json")
        print("   â€¢ milli_mucadele.json")
        print("   â€¢ cumhuriyet.json")
        return
    
    # JSON dosyasÄ± var mÄ± kontrol et
    json_files = list(data_dir.glob('*.json'))
    if not json_files:
        print("âš ï¸  data/raw klasÃ¶rÃ¼nde JSON dosyasÄ± bulunamadÄ±!")
        print("\nğŸ“‹ Beklenen dosyalar:")
        print("   â€¢ islamiyet_oncesi.json")
        print("   â€¢ ilk_turk_islam_devletleri.json")
        print("   â€¢ anadolu_beylikleri.json")
        print("   â€¢ osmanli_devleti.json")
        print("   â€¢ milli_mucadele.json")
        print("   â€¢ cumhuriyet.json")
        return
    
    # Veri iÅŸleme pipeline'Ä±nÄ± baÅŸlat
    processor = DataProcessor()
    processor.process_all()


if __name__ == "__main__":
    main()