"""
Retrieval Ä°ÅŸlemleri ModÃ¼lÃ¼
FAISS ile similarity search ve retrieval iÅŸlemleri
"""

import faiss
import numpy as np
from typing import List, Dict, Tuple
import json
from pathlib import Path


class FAISSRetriever:
    """FAISS tabanlÄ± retrieval sÄ±nÄ±fÄ±"""
    
    def __init__(self, index_path: str = "models/faiss_index", dimension: int = 384):
        """
        Args:
            index_path: FAISS index dizini
            dimension: Embedding vektÃ¶r boyutu
        """
        self.index_path = Path(index_path)
        self.dimension = dimension
        self.index = None
        self.chunk_list = []
        
        # Index varsa yÃ¼kle
        if (self.index_path / "index.faiss").exists():
            self.load_index()
    
    def create_index(self, embeddings: np.ndarray, metadata: List[Dict] = None):
        """
        Yeni FAISS index oluÅŸtur
        
        Args:
            embeddings: Embedding vektÃ¶rleri matrisi (n_docs x dimension)
            metadata: Her embedding iÃ§in metadata listesi
        """
        print("ğŸ”§ FAISS index oluÅŸturuluyor...")
        
        # L2 (Euclidean) distance yerine IndexFlatIP (Inner Product) kullan
        self.index = faiss.IndexFlatIP(self.dimension)
        
        # Embeddings'i ekle
        embeddings = embeddings.astype('float32')
        # Not: data_processing.py'nin vektÃ¶rleri L2-normalize etmesi GEREKÄ°R
        self.index.add(embeddings)
        
        print(f"âœ… Index oluÅŸturuldu (toplam vektÃ¶r: {self.index.ntotal})")
        
        # Metadata'yÄ± sakla
        if metadata:
           self.chunk_list = metadata
    
    def save_index(self, metadata_list: List[Dict] = None):
        """
        Index'i diske kaydet
        
        Args:
            metadata_list: Kaydedilecek metadata listesi
        """
        self.index_path.mkdir(parents=True, exist_ok=True)
        
        # FAISS index'i kaydet
        index_file = self.index_path / "index.faiss"
        faiss.write_index(self.index, str(index_file))
        print(f"ğŸ’¾ FAISS index kaydedildi: {index_file}")
        
        # Metadata'yÄ± kaydet
        if metadata_list or self.chunk_list:
            metadata_file = self.index_path / "metadata.json"
            metadata_to_save = metadata_list if metadata_list else self.chunk_list
            
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata_to_save, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ Metadata (chunk listesi) kaydedildi: {metadata_file}")
    
    def load_index(self):
        """Index'i diskten yÃ¼kle"""
        index_file = self.index_path / "index.faiss"
        metadata_file = self.index_path / "metadata.json"
        
        if not index_file.exists():
            raise FileNotFoundError(f"Index dosyasÄ± bulunamadÄ±: {index_file}")
        
        # FAISS index'i yÃ¼kle
        self.index = faiss.read_index(str(index_file))
        print(f"ğŸ“‚ FAISS index yÃ¼klendi: {self.index.ntotal} vektÃ¶r")
        
        # Metadata'yÄ± yÃ¼kle
        if metadata_file.exists():
            with open(metadata_file, 'r', encoding='utf-8') as f:
                self.chunk_list = json.load(f)  # Bu artÄ±k bir liste
            print(f"ğŸ“‚ Metadata yÃ¼klendi: {len(self.chunk_list)} kayÄ±t")
    
    def search(self, query_embedding: np.ndarray, top_k: int = 5) -> Tuple[np.ndarray, np.ndarray]:
        """
        Query iÃ§in benzer vektÃ¶rleri bul
        
        Args:
            query_embedding: Query embedding vektÃ¶rÃ¼
            top_k: DÃ¶ndÃ¼rÃ¼lecek sonuÃ§ sayÄ±sÄ±
            
        Returns:
            (distances, indices) tuple'Ä±
        """
        if self.index is None:
            raise ValueError("Index yÃ¼klenmemiÅŸ! Ã–nce create_index veya load_index Ã§aÄŸÄ±rÄ±n.")
        
        # Query'yi uygun formata Ã§evir
        query_embedding = query_embedding.astype('float32').reshape(1, -1)

        # IndexFlatIP iÃ§in sorgu vektÃ¶rÃ¼nÃ¼ normalize et ***
        faiss.normalize_L2(query_embedding)
        
        # Arama yap
        # IndexFlatIP kullanÄ±ldÄ±ÄŸÄ±nda, 'distances' aslÄ±nda 'similarity scores' (benzerlik skorlarÄ±) olur
        similarities, indices = self.index.search(query_embedding, top_k)
        
        return similarities[0], indices[0]
    
    def retrieve(self, query_embedding: np.ndarray, 
                top_k: int = 5, 
                threshold: float = None) -> List[Dict]:
        """
        Query iÃ§in alakalÄ± dokÃ¼manlarÄ± al
        
        Args:
            query_embedding: Query embedding vektÃ¶rÃ¼
            top_k: DÃ¶ndÃ¼rÃ¼lecek sonuÃ§ sayÄ±sÄ±
            threshold: Minimum benzerlik eÅŸiÄŸi (opsiyonel)
            
        Returns:
            AlakalÄ± dokÃ¼manlarÄ±n metadata listesi
        """
        # Arama yap
        similarities, indices = self.search(query_embedding, top_k)
        
        # SonuÃ§larÄ± hazÄ±rla
        results = []
        for similarity_score, idx in zip(similarities, indices):
            
            # Threshold kontrolÃ¼
            if threshold and similarity_score < threshold:
                continue
            
            # Metadata'yÄ± ekle
            result = {
                "index": int(idx),
                "distance": float(1 - similarity_score), # Mesafeyi 1-similarity olarak hesapla
                "similarity": float(similarity_score)
            }
            
            # Metadata varsa ekle
            if self.chunk_list and idx < len(self.chunk_list):
                result.update(self.chunk_list[idx])
            
            results.append(result)
        
        return results
    
    def add_vectors(self, new_embeddings: np.ndarray, new_metadata: List[Dict] = None):
        """
        Mevcut index'e yeni vektÃ¶rler ekle
        
        Args:
            new_embeddings: Yeni embedding vektÃ¶rleri
            new_metadata: Yeni metadata listesi
        """
        if self.index is None:
            raise ValueError("Index yÃ¼klenmemiÅŸ!")
        
        # VektÃ¶rleri ekle
        new_embeddings = new_embeddings.astype('float32')
        # *** Yeni vektÃ¶rleri de normalize et ***
        faiss.normalize_L2(new_embeddings)
        self.index.add(new_embeddings)
        
        # Metadata'yÄ± gÃ¼ncelle
        if new_metadata and self.chunk_list is not None:
            self.chunk_list.extend(new_metadata)
        
        print(f"âœ… {len(new_embeddings)} yeni vektÃ¶r eklendi (toplam: {self.index.ntotal})")
    
    def get_stats(self) -> Dict:
        """
        Index istatistiklerini dÃ¶ndÃ¼r
        
        Returns:
            Ä°statistik dictionary
        """
        return {
            "total_vectors": self.index.ntotal if self.index else 0,
            "dimension": self.dimension,
            "index_type": type(self.index).__name__ if self.index else None,
            "metadata_count": len(self.chunk_list) if self.chunk_list else 0
        }


class HybridRetriever:
    """Hybrid retrieval (semantic + keyword) sÄ±nÄ±fÄ±"""
    
    def __init__(self, faiss_retriever: FAISSRetriever, alpha: float = 0.7):
        """
        Args:
            faiss_retriever: FAISS retriever instance
            alpha: Semantic search aÄŸÄ±rlÄ±ÄŸÄ± (0-1 arasÄ±)
        """
        self.faiss_retriever = faiss_retriever
        self.alpha = alpha  # Semantic weight
        self.beta = 1 - alpha  # Keyword weight
    
    def keyword_search(self, query: str, documents: List[str], top_k: int = 5) -> List[Tuple[int, float]]:
        """
        Basit keyword tabanlÄ± arama
        
        Args:
            query: Arama sorgusu
            documents: DokÃ¼man listesi
            top_k: DÃ¶ndÃ¼rÃ¼lecek sonuÃ§ sayÄ±sÄ±
            
        Returns:
            (index, score) tuple'larÄ±nÄ±n listesi
        """
        query_words = set(query.lower().split())
        
        scores = []
        for idx, doc in enumerate(documents):
            doc_words = set(doc.lower().split())
            
            # Jaccard similarity
            intersection = len(query_words.intersection(doc_words))
            union = len(query_words.union(doc_words))
            
            score = intersection / union if union > 0 else 0
            scores.append((idx, score))
        
        # Skora gÃ¶re sÄ±rala
        scores.sort(key=lambda x: x[1], reverse=True)
        return scores[:top_k]
    
    def hybrid_search(self, query_embedding: np.ndarray, 
                     query_text: str,
                     documents: List[str],
                     top_k: int = 5) -> List[Dict]:
        """
        Hybrid search: semantic + keyword
        
        Args:
            query_embedding: Query embedding vektÃ¶rÃ¼
            query_text: Query metni
            documents: DokÃ¼man listesi
            top_k: DÃ¶ndÃ¼rÃ¼lecek sonuÃ§ sayÄ±sÄ±
            
        Returns:
            SonuÃ§ listesi
        """
        # Semantic search
        semantic_results = self.faiss_retriever.retrieve(query_embedding, top_k=top_k*2)
        
        # Keyword search
        keyword_results = self.keyword_search(query_text, documents, top_k=top_k*2)
        
        # SkorlarÄ± birleÅŸtir
        combined_scores = {}
        
        for result in semantic_results:
            idx = result['index']
            combined_scores[idx] = self.alpha * result['similarity']
        
        for idx, score in keyword_results:
            if idx in combined_scores:
                combined_scores[idx] += self.beta * score
            else:
                combined_scores[idx] = self.beta * score
        
        # SÄ±rala ve top-k'yÄ± al
        sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]
        
        # SonuÃ§larÄ± formatla
        final_results = []
        for idx, score in sorted_results:
            result = {
                "index": idx,
                "hybrid_score": score,
                "document": documents[idx] if idx < len(documents) else None
            }
            final_results.append(result)
        
        return final_results


def test_retrieval():
    """Retrieval modÃ¼lÃ¼nÃ¼ test et"""
    print("\n" + "="*60)
    print("RETRIEVAL MODÃœLÃœ TEST (IndexFlatIP)")
    print("="*60 + "\n")
    
    # Test embeddings oluÅŸtur
    print("ğŸ”§ Test embeddings oluÅŸturuluyor...")
    dimension = 384
    n_docs = 10
    
    # Random embeddings (test iÃ§in)
    np.random.seed(42)
    embeddings = np.random.randn(n_docs, dimension).astype('float32')

    # Normalize et (IP iÃ§in ZORUNLU)
    faiss.normalize_L2(embeddings)
    
    # Metadata oluÅŸtur (ArtÄ±k chunk listesi formatÄ±nda)
    metadata = [
        {"content": f"Test dokÃ¼manÄ± {i+1}", "metadata": {"source": f"doc_{i+1}.txt"}}
        for i in range(n_docs)
    ]
    
    # Retriever oluÅŸtur
    retriever = FAISSRetriever(index_path="models/test_index", dimension=dimension)
    
    # Index oluÅŸtur
    retriever.create_index(embeddings, metadata)
    
    # Index'i kaydet
    retriever.save_index() # ArtÄ±k metadata'yÄ± (chunk_list) doÄŸru kaydediyor
    
    # Test query
    print("\nğŸ” Test query oluÅŸturuluyor...")
    query_embedding = np.random.randn(dimension).astype('float32')
    
    # Arama yap
    print("\nğŸ“Š Arama yapÄ±lÄ±yor...")
    results = retriever.retrieve(query_embedding, top_k=5)
    
    print(f"âœ… {len(results)} sonuÃ§ bulundu:\n")
    for i, result in enumerate(results, 1):
        print(f"{i}. DokÃ¼man: {result.get('content', 'N/A')}")
        print(f"   Similarity: {result['similarity']:.4f}") # ArtÄ±k bu direkt KosinÃ¼s BenzerliÄŸi
        print(f"   Distance: {result['distance']:.4f}\n") # Bu (1 - Similarity)
    
    # Ä°statistikler
    stats = retriever.get_stats()
    print("ğŸ“ˆ Index Ä°statistikleri:")
    for key, value in stats.items():
        print(f"  â€¢ {key}: {value}")
    
    print("\n" + "="*60)
    print("âœ… TEST TAMAMLANDI")
    print("="*60 + "\n")


if __name__ == "__main__":
    test_retrieval()